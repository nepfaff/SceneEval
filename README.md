# SceneEval

### SceneEval: Evaluating Semantic Coherence in Text-Conditioned 3D Indoor Scene Synthesis

[Hou In Ivan Tam](https://iv-t.github.io/), [Hou In Derek Pun](https://houip.github.io/), [Austin T. Wang](https://atwang16.github.io/), [Angel X. Chang](https://angelxuanchang.github.io/), [Manolis Savva](https://msavva.github.io/)

<!-- <img src="docs/static/images/teaser.webp" alt="teaser" style="width:100%"/> -->

[Page](https://3dlg-hcvc.github.io/SceneEval/) | [Paper](https://arxiv.org/abs/2503.14756) | [Data](https://github.com/3dlg-hcvc/SceneEval/releases)



## News
- 2025-11-26: Added [SceneWeaver](https://github.com/princeton-vl/SceneWeaver) support with proper Infinigen object decomposition!
- 2025-11-04: Released scripts for converting Holodeck output into SceneEval-compatible format!
- 2025-10-27: Release v1.1 with a new metric *Opening Clearance*, support for [LayoutVLM](https://github.com/sunfanyunn/LayoutVLM) and [HSM](https://github.com/3dlg-hcvc/hsm), bug fixes, and more! The environment setup is now simplified and the demo is easier to run! Give it a try!
- 2025-06-27: Codebase release v1.0!
- 2025-06-10: Released SceneEval-500 dataset and v0.9 of the SceneEval codebase!



## Todo List
- [x] Add documentation for the scene state format
- [x] Provide script for downloading and processing Holodeck's assets
- [x] Create guide for extending SceneEval with new methods and metrics
- [ ] Replace custom VLM interface with Pydantic AI



## Quick Start

### 1. Install dependencies with uv

```bash
uv sync
source .venv/bin/activate
```

### 2. Download data

Run the setup script to download all available datasets:

```bash
./setup.sh
```

This will:
- Download SceneEval-500 annotations
- Download Objathor assets (for Holodeck) - ~50GB
- Download HSSD assets (for HSM) - ~80GB (requires `gltf-transform` and `ktx`)

**Manual downloads required** (due to licensing):
- **3D-FUTURE** (for ATISS, DiffuScene, LayoutGPT, InstructScene): [Download here](https://tianchi.aliyun.com/specials/promotion/alibaba-3d-future) → extract to `_data/3D-FUTURE-model/`
- **LayoutVLM assets**: [Download here](https://github.com/sunfanyunn/LayoutVLM#data-preprocessing) → extract to `_data/layoutvlm-objathor/`

### 3. (Optional) Setup OpenAI API Key

For VLM-dependent metrics (*Object Count*, *Object Attribute*, *Object-Object Relationship*, *Object-Architecture Relationship*, *Object Support*, *Object Accessibility*), create a `.env` file:
```
OPENAI_API_KEY=<your_key>
```

Metrics that do NOT require a VLM: *Collision*, *Navigability*, *Out of Bounds*, *Opening Clearance*, *Drake Collision*, *Static Equilibrium*, *Welded Equilibrium*

---

<details>
<summary><strong>Alternative: Install with conda</strong></summary>

```bash
conda env create -f environment.yaml
conda activate scene_eval
```

</details>

<details>
<summary><strong>HSSD prerequisites (for HSM support)</strong></summary>

To download HSSD assets, you need:
1. [Agree to the HSSD dataset license on HuggingFace](https://huggingface.co/datasets/hssd/hssd-models)
2. Install tools:
   - [gltf-transform](https://www.npmjs.com/package/@gltf-transform/cli): `npm install -g @gltf-transform/cli`
   - [ktx](https://github.com/KhronosGroup/KTX-Software/releases) from KhronosGroup

</details>

<details>
<summary><strong>Expected <code>_data/</code> directory structure</strong></summary>

```
_data
├── 3D-FUTURE-model
│   ├── model_info.json
│   ├── 0a0f0cf2-3a34-4ba2-b24f-34f361c36b3e
│   |   ├── raw_model.obj
│   |   ├── model.mtl
│   |   ├── texture.png
│   |   ├── ...
│   ├── ...
├── objathor-assets
│   ├── annotations.json
│   ├── 0a0a8274693445a6b533dce7f97f747c
│   |   ├── 0a0a8274693445a6b533dce7f97f747c.glb
│   |   ├── ...
├── layoutvlm-objathor
│   ├── 0a3dc72fb1bb41439005cac4a3ebb765
│   |   ├── 0a3dc72fb1bb41439005cac4a3ebb765.glb
│   |   ├── data.json
│   |   ├── ...
│   ├── ...
└── hssd
    ├── fpmodels.csv
    ├── glb
     │   ├── 0
     │   |   ├── 0a0b9f607345b6cee099d681f28e19e1f0a215c8.glb
     │   |   ├── ...
     │   ├── ...
     └── decomposed
         ├── 00a2b0f3886ccb5ffddac704f8eeec324a5e14c6
         |   ├── 00a2b0f3886ccb5ffddac704f8eeec324a5e14c6_part_1.glb
         |   ├── 00a2b0f3886ccb5ffddac704f8eeec324a5e14c6_part_2.glb
         |   ├── ...
         ├── ...
```

</details>


## Quick Start Demo

Try SceneEval with our provided example scenes. You do *not* need an OpenAI API key for this demo.

### 1. Download the LayoutVLM Assets
Follow the instructions in the [*For LayoutVLM*](#layoutvlm) section above to download the LayoutVLM assets.

### 2. Run the Demo
```bash
# Copy provided example scenes to input directory
cp -r input_example/* input/

# Run SceneEval
python main.py
```
This will run the evaluation on five example scenes generated by LayoutVLM using the `no_llm_plan` evaluation plan, which runs the following metrics: *Collision*, *Navigability*, *Out of Bounds*, and *Opening Clearance*.

Results will be saved to `./output_eval`.



## Extending SceneEval to a New Method or Dataset

SceneEval is built to be extensible! You can easily add new scene generation methods, evaluation metrics, and assets.

**[Follow this step-by-step guide to see how to add a new method that uses a new 3D asset source.](./GUIDE.md)**



## Contributing to SceneEval

Found a bug or want to contribute a new method or metric? We'd love your help! Please open an issue or submit a pull request. 



## Citation
If you find SceneEval helpful in your research, please cite our work:
```
@article{tam2025sceneeval,
    title = {{SceneEval}: Evaluating Semantic Coherence in Text-Conditioned {3D} Indoor Scene Synthesis},
    author = {Tam, Hou In Ivan and Pun, Hou In Derek and Wang, Austin T. and Chang, Angel X. and Savva, Manolis},
    year = {2025},
    eprint = {2503.14756},
    archivePrefix = {arXiv}
}
```

**Note:** When using SceneEval in your work, we encourage you to specify which version of the code and dataset you are using (e.g., commit hash, release tag, or dataset version) to ensure reproducibility and proper attribution.

## SceneWeaver Support

SceneEval supports evaluating scenes generated by [SceneWeaver](https://github.com/princeton-vl/SceneWeaver). The conversion script properly handles Infinigen's procedural object factories, exporting each object individually (including bed components like mattress, pillows, comforter, etc.) rather than bundling them.

### Converting SceneWeaver Output

Use Blender to run the conversion script:

```bash
blender --background --python conversion/sceneweaver/convert_SceneEval.py -- \
    --input_dir /path/to/SceneWeaver/Pipeline/output/Design_me_a_messy_kids_bedroom_0 \
    --output_dir ./input/SceneWeaver \
    --scene_id 0
```

**Arguments:**
- `--input_dir`: Path to SceneWeaver output directory (contains `record_files/`, `record_scene/`, `roominfo.json`)
- `--output_dir`: Path to SceneEval input directory (default: `./input/SceneWeaver`)
- `--scene_id`: Scene ID for output filename (default: 0)
- `--wall_height`: Wall height in meters (default: 2.8)

### Running Evaluation

After conversion, run SceneEval:

```bash
python main.py \
    evaluation_plan.input_cfg.scene_methods=[SceneWeaver] \
    evaluation_plan.input_cfg.scene_mode=range \
    evaluation_plan.input_cfg.scene_range=[0,1]
```

Results will be saved to `./output_eval/SceneWeaver/`.

### How It Works

The conversion script:
1. Opens the SceneWeaver Blender file (`.blend`)
2. Finds all objects matching the `*.spawn_asset(*` pattern (Infinigen factory objects)
3. Exports each object as an individual GLB file with world-space transforms
4. Creates a SceneEval scene state JSON with proper architecture (floor, walls)

This approach matches SceneWeaver's own object counting methodology (`Nobj_unique` in `metric_*.json`), ensuring fair evaluation.


## Drake Physics Metrics

SceneEval includes physics-based metrics using [Drake](https://drake.mit.edu/) for more accurate collision detection and stability analysis.

### Available Metrics

| Metric | Description |
|--------|-------------|
| **DrakeCollisionMetric** | Detects collisions using [CoACD](https://github.com/SarahWeiii/CoACD) convex decomposition for physics-accurate collision geometry. More accurate than trimesh-based collision for physics simulation. |
| **StaticEquilibriumMetric** | Runs physics simulation and measures object displacement. Lower displacement = better stability. |
| **WeldedEquilibriumMetric** | Detects penetrating objects, welds them to the world, then simulates. Isolates stability issues from penetration-induced movement. |

### Why Drake-based Collision Detection?

The original `CollisionMetric` uses trimesh intersection which checks exact mesh geometry. However, physics simulators like Drake use **convex collision geometry** (via CoACD decomposition), which can slightly inflate meshes. `DrakeCollisionMetric` detects collisions that would actually occur in physics simulation.

### Output Statistics

`DrakeCollisionMetric` reports:
- `max_penetration_depth` - Maximum penetration depth across all collision pairs
- `min_penetration_depth` - Minimum penetration depth
- `mean_penetration_depth` - Average penetration depth
- `median_penetration_depth` - Median penetration depth
- `num_collision_pairs` - Number of unique colliding object pairs
- `num_obj_in_collision` - Number of objects involved in collisions

`StaticEquilibriumMetric` and `WeldedEquilibriumMetric` report:
- `scene_stable` - Whether all objects are stable (displacement < threshold)
- `mean_displacement` - Average object displacement after simulation
- `max_displacement` - Maximum object displacement
- `per_object_results` - Displacement and rotation per object

### Usage

```bash
# Run Drake collision metric
python main.py \
    'evaluation_plan.input_cfg.scene_methods=[LayoutVLM]' \
    'evaluation_plan.evaluation_cfg.metrics=[DrakeCollisionMetric]'

# Run all Drake physics metrics
python main.py \
    'evaluation_plan.input_cfg.scene_methods=[LayoutVLM]' \
    'evaluation_plan.evaluation_cfg.metrics=[DrakeCollisionMetric,StaticEquilibriumMetric,WeldedEquilibriumMetric]'
```


## Acknowledgements
This work was funded in part by the Sony Research Award Program, a CIFAR AI Chair, a Canada Research Chair, NSERC Discovery Grants, and enabled by support from the [Digital Research Alliance of Canada](https://alliancecan.ca/).
We thank Nao Yamato, Yotaro Shimose, and other members on the Sony team for their feedback.
We also thank Qirui Wu, Xiaohao Sun, and Han-Hung Lee for helpful discussions.
